{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "b2687165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'de_standardize' from 'helpers' (/Users/salma/Desktop/ML/ML_course-master/labs/ex02/template/helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [297], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m de_standardize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#sfrom gradient_descent import *\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#from test_utils import test\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'de_standardize' from 'helpers' (/Users/salma/Desktop/ML/ML_course-master/labs/ex02/template/helpers.py)"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "from helpers import de_standardize\n",
    "from helpers import *\n",
    "#sfrom gradient_descent import *\n",
    "#from test_utils import test\n",
    "from ipywidgets import IntSlider, interact\n",
    "from plots import gradient_descent_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.array([0, 0])\n",
    "max_iters = 50\n",
    "lambda_s = np.logspace(-5, 0, 15)\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e = y - tx.dot(w)\n",
    "    dim = e.shape[0] \n",
    "    return (1/2) * (1/dim)* e.dot(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    loss = compute_loss(y,tx,w)\n",
    "    e = y - tx.dot(w)\n",
    "    dim = e.shape[0]\n",
    "    grad = - 1/dim * (tx.T).dot(e)\n",
    "    result= np.array([grad,loss])\n",
    "    return result\n",
    "    \n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0352c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape= (N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        grad = compute_gradient(y, tx, w)[0]\n",
    "        loss = compute_gradient(y,tx,w)[1]\n",
    "      \n",
    "\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "\n",
    "    loss = compute_loss(y,tx,w)\n",
    "    e = y - tx.dot(w)\n",
    "    dim = e.shape[0]\n",
    "    grad = - 1/dim * (tx.T).dot(e)\n",
    "    result= np.array([grad,loss])\n",
    "    return result  \n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    grad = compute_stoch_gradient(y, tx, w)\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        for y_mb, tx_mb in batch_iter(y, tx, batch_size):\n",
    "            grad = compute_stoch_gradient(y_mb, tx_mb, w)[0]\n",
    "            w = w - gamma * grad\n",
    "            loss = compute_stoch_gradient(y_mb, tx_mb, w)[1]\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca17c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regession using gradient descent \n",
    "def least_squares_GD(y,tx,initial_w,max_iters,gamma):\n",
    "\n",
    "# Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data\n",
    "# ***************************************************\n",
    "    gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\"\"\"# Time Visualization\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cae4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression using SGD\n",
    "def least_squares_SGD(y,tx,initial_w,max_iters,gamma):\n",
    "    \n",
    "    batch_size = 1\n",
    "   # Start SGD.\n",
    "    start_time = datetime.datetime.now()\n",
    "    sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "\"\"\"# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#least squares regression  \n",
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "\n",
    "    >>> least_squares(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]))\n",
    "    (array([ 0.21212121, -0.12121212]), 8.666684749742561e-33)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    # ***************************************************\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a,b)\n",
    "    mse = compute_loss(y,tx,w)\n",
    "    return w,mse\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93152f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 0)\n",
    "    array([ 0.21212121, -0.12121212])\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 1)\n",
    "    array([0.03947092, 0.00319628])\n",
    "    \"\"\"\n",
    "    a = tx.T.dot(tx)+ 2*tx.shape[0]*lambda_*np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a,b)  \n",
    "    return w                   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_implem(y,tx,lambdas):\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        print (ridge_regression(y,tx,lambda_))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e832ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "   \n",
    "    return -np.sum(y * np.log(sigmoid(tx.T.dot(w))) + (1 - y) * np.log(1 - sigmoid(tx.T.dot(w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # compute the cost: TODO\n",
    "    # ***************************************************\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # compute the gradient: TODO\n",
    "    # ***************************************************\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # update w: TODO\n",
    "    # ***************************************************\n",
    "    w = w - gamma*gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import de_standardize\n",
    "#Logistic regression using gradient descent or SGD(y∈{0,1}\n",
    "def logistic_regression(y,tx,initial_w,max_iters=10000,gamma = 0.01):\n",
    "    # init parameters\n",
    "    #max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def reg_logistic_regression(y, tx, lambda,initialw, maxiters, gamma):\n",
    "# Regularized  logistic  regression  using  gradient  descentor SGD (y∈ {0,1}, with regularization termλ∥w∥2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d844c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_method():\n",
    "    height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "    x, mean_x, std_x = standardize(height)\n",
    "    y, tx = build_model_data(x, weight)\n",
    "    weight = logistic_regression(y,tx,initial_w)\n",
    "    #ridge_regression_implem(y, tx, lambda_s)\n",
    "    #least_squares_GD(y,tx,initial_w,max_iters,gamma)\n",
    "    #least_squares_SGD(y,tx,initial_w,max_iters,gamma)\n",
    "    #least_squares(y, tx)[0]\n",
    "    print(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75257ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32850ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load-data /preprocessing/split\n",
    "# binary classification "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
