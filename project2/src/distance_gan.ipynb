{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f92e89d1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "import torchvision \n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "torch.set_num_threads(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameter:\n",
    "    num_classes: int        = 1\n",
    "    batchsize: int          = 128\n",
    "    num_epochs: int         = 20\n",
    "    latent_size: int        = 32 #used in generatinf random noise?\n",
    "    n_critic: int           = 5\n",
    "    critic_size: int        = 3\n",
    "    generator_size: int     = 2\n",
    "    critic_hidden_size: int = 1024\n",
    "    gp_lambda: float        = 10. #used in gradient\n",
    "        \n",
    "hp = Hyperparameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 256),# 2-input generator (noise, energy: en_p)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1), #1 output distance\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), 100)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.view(x.size(0), 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(1, 100)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 1024), # 3-input discriminator (real data = d_p,label=,en_p,generated data = distance)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() #1-output discriminator (real vs fake/generated)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = x.view(128)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize Generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    \n",
    "discriminator_optimizer = optim.AdamW(discriminator.parameters(), lr=1e-4,betas=(0., 0.9))\n",
    "generator_optimizer = optim.AdamW(generator.parameters(), lr=1e-4,betas=(0., 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_df = pd.read_pickle(\"water_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = water_df[\"en_p\"].values.reshape(-1,1)\n",
    "#dependent variable distance\n",
    "y = water_df[\"dist_p\"].values.reshape(-1,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method count of tuple object at 0x000001F9456FDC40>\n",
      "the data is ok\n"
     ]
    }
   ],
   "source": [
    "# determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)\n",
    "\n",
    "device = get_device()\n",
    "device = get_device()\n",
    "x_train_torch = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_torch = torch.from_numpy(y_train).float().to(device)\n",
    "trainset = torch.utils.data.TensorDataset(x_train_torch, y_train_torch)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "print('the data is ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(hp\u001b[39m.\u001b[39mnum_epochs):\n\u001b[0;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m         real_images, real_class_labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m), all_labels[data[\u001b[39m1\u001b[39;49m]]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m         \u001b[39m# Update critic\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         discriminator_optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "\n",
    "img_list, generator_losses, discriminator_losses = [], [], []\n",
    "iters = 0\n",
    "all_labels = torch.eye(hp.num_classes, dtype=torch.float32, device=\"cuda\")\n",
    "fixed_noise = torch.randn((80, hp.latent_size), device=\"cuda\")\n",
    "fixed_class_labels = all_labels[[i for i in list(range(hp.num_classes)) for idx in range(8)]]\n",
    "grad_tensor = torch.ones((hp.batchsize, 1), device=\"cuda\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(hp.num_epochs):\n",
    "    for batch_idx, data in enumerate(trainloader, 0):\n",
    "        real_images, real_class_labels = data[0].to(\"cuda\"), all_labels[data[1]].to(\"cuda\")\n",
    "        \n",
    "        # Update critic\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        \n",
    "        discriminator_output_real = discriminator(real_images, real_class_labels)\n",
    "        discriminator_loss_real = discriminator_output_real.mean()\n",
    "\n",
    "        noise = torch.randn((hp.batchsize, hp.latent_size), device=\"cuda\")\n",
    "        with torch.no_grad(): fake_image = generator(noise, real_class_labels)\n",
    "        discriminator_output_fake = discriminator(fake_image, real_class_labels)\n",
    "        discriminator_loss_fake = discriminator_output_fake.mean()\n",
    "\n",
    "        alpha = torch.rand((hp.batchsize, 1), device=\"cuda\")\n",
    "        interpolates = (alpha.view(-1, 1, 1, 1) * real_images + ((1. - alpha.view(-1, 1, 1, 1)) * fake_image)).requires_grad_(True)\n",
    "        d_interpolates = discriminator(interpolates, real_class_labels)\n",
    "        gradients = autograd.grad(d_interpolates, interpolates, grad_tensor, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_penalty = hp.gp_lambda * ((gradients.view(hp.batchsize, -1).norm(dim=1) - 1.) ** 2).mean()\n",
    "\n",
    "        discriminator_loss = -discriminator_loss_real + discriminator_loss_fake  + gradient_penalty\n",
    "        \n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        if batch_idx % hp.n_critic == 0:\n",
    "            # Update Generator\n",
    "            generator_optimizer.zero_grad()\n",
    "            \n",
    "            fake_class_labels = all_labels[torch.randint(hp.num_classes, size=[hp.batchsize])]\n",
    "            noise = torch.randn((hp.batchsize, hp.latent_size), device=\"cuda\")\n",
    "            fake_image = generator(noise, fake_class_labels)\n",
    "            discriminator_output_fake = discriminator(fake_image, fake_class_labels)\n",
    "            generator_loss = -discriminator_output_fake.mean()\n",
    "            \n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_idx % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"[{epoch:>2}/{hp.num_epochs}][{iters:>7}][{elapsed_time:8.2f}s]\\t\"\n",
    "                  f\"d_loss/g_loss: {discriminator_loss.item():4.2}/{generator_loss.item():4.2}\\t\")\n",
    "       \n",
    "        # Save Losses for plotting later\n",
    "        generator_losses.append(generator_loss.item())\n",
    "        discriminator_losses.append(discriminator_loss.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == hp.num_epochs - 1) and (batch_idx == len(trainloader) - 1)):\n",
    "            with torch.no_grad(): fake_images = generator(fixed_noise, fixed_class_labels).cpu()\n",
    "            img_list.append(vutils.make_grid(fake_images, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b929b525c3ad5b5165d0d1ea767dbde287d9c26320fa3889726f679d0a1bb497"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
