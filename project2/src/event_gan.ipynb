{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ImmyKcwQPvJB",
    "outputId": "afe0e42e-4f47-4c68-dcef-f6e98146c1b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f03d826f030>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim, autograd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.set_num_threads(10)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Version: 3.10.7\n",
      "torch Version: 1.13.0+cu117\n",
      "torchvision Version: 0.14.0+cu117\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"python Version: {sys.version.split(' ')[0]}\")\n",
    "print(f\"torch Version: {torch.__version__}\")\n",
    "print(f\"torchvision Version: {torchvision.__version__}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nitbSS77Px4X"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameter:\n",
    "    batchsize: int          = 128\n",
    "    num_epochs: int         = 20\n",
    "    noise_size: int         = 4\n",
    "    n_critic: int           = 5\n",
    "    gp_lambda: float        = 10.\n",
    "        \n",
    "hp = Hyperparameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "u05eDLLwbYAo",
    "outputId": "20775a1f-2619-42b8-e5e0-7dfba19e4f0f"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(hp.noise_size + 2, 256), # input: (noise, dist_p, en_p)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 4), # output: (cos_p, de_p, cos_c, en_c)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        out = self.model(torch.cat([noise, labels], dim=1))\n",
    "        return out.squeeze()\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(6, 1024), # input: (cos_p, de_p, cos_c, en_c, dist_p, en_p)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, event, labels):\n",
    "        out = self.model(torch.cat([event, labels], dim=1))\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ParticlesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path=\"../pickled_data/water_dataset.pkl\", test_size=0.3):\n",
    "        dataset = pd.read_pickle(path)\n",
    "        dataset = dataset[dataset[\"emission\"] == 1]\n",
    "\n",
    "        x = dataset[[\"cos_p\", \"de_p\", \"cos_c\", \"en_c\"]]\n",
    "        y = dataset[[\"dist_p\", \"en_p\"]]\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
    "\n",
    "        self.x_train=torch.from_numpy(x_train.values).float()\n",
    "        self.y_train=torch.from_numpy(y_train.values).float()\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGaEIpNJbHEJ"
   },
   "outputs": [],
   "source": [
    "dataset = ParticlesDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=hp.batchsize, num_workers=1, shuffle=True, drop_last=True, pin_memory=True)\n",
    "critic, generator = Critic().to(\"cuda\"), Generator().to(\"cuda\")\n",
    "\n",
    "critic_optimizer = optim.AdamW(critic.parameters(), lr=1e-4, betas=(0., 0.9))\n",
    "generator_optimizer = optim.AdamW(generator.parameters(), lr=1e-4, betas=(0., 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AKkYiOC2P9mi",
    "outputId": "238aca44-9928-40df-f20f-45615efe7853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0/20][      0][    0.26s]\td_loss/g_loss:  9.9/-0.17\t\n",
      "[ 0/20][    100][    0.78s]\td_loss/g_loss:  8.5/-0.68\t\n",
      "[ 0/20][    200][    1.33s]\td_loss/g_loss:  7.8/-0.76\t\n",
      "[ 0/20][    300][    1.86s]\td_loss/g_loss:  7.5/-0.89\t\n",
      "[ 0/20][    400][    2.54s]\td_loss/g_loss:  8.0/-0.42\t\n",
      "[ 0/20][    500][    3.21s]\td_loss/g_loss:  6.8/-0.47\t\n",
      "[ 0/20][    600][    4.00s]\td_loss/g_loss:  6.3/-0.65\t\n",
      "[ 0/20][    700][    4.71s]\td_loss/g_loss:  5.9/-0.54\t\n",
      "[ 0/20][    800][    5.48s]\td_loss/g_loss:  6.1/-0.71\t\n",
      "[ 0/20][    900][    6.20s]\td_loss/g_loss:  5.7/-0.48\t\n",
      "[ 0/20][   1000][    6.87s]\td_loss/g_loss:  6.0/-0.81\t\n",
      "[ 0/20][   1100][    7.58s]\td_loss/g_loss:  5.7/-0.54\t\n",
      "[ 0/20][   1200][    8.14s]\td_loss/g_loss:  5.9/-0.49\t\n",
      "[ 0/20][   1300][    8.76s]\td_loss/g_loss:  5.7/-0.56\t\n",
      "[ 0/20][   1400][    9.31s]\td_loss/g_loss:  6.1/-0.72\t\n",
      "[ 0/20][   1500][    9.85s]\td_loss/g_loss:  6.5/-0.5\t\n",
      "[ 0/20][   1600][   10.34s]\td_loss/g_loss:  5.0/-0.45\t\n",
      "[ 0/20][   1700][   10.95s]\td_loss/g_loss:  4.6/-0.7\t\n",
      "[ 0/20][   1800][   11.49s]\td_loss/g_loss:  5.7/-0.75\t\n",
      "[ 0/20][   1900][   11.93s]\td_loss/g_loss:  6.8/-0.54\t\n",
      "[ 0/20][   2000][   12.49s]\td_loss/g_loss:  5.7/-0.19\t\n",
      "[ 0/20][   2100][   13.07s]\td_loss/g_loss:  4.9/-0.63\t\n",
      "[ 0/20][   2200][   13.66s]\td_loss/g_loss:  4.4/-0.53\t\n",
      "[ 0/20][   2300][   14.22s]\td_loss/g_loss:  4.7/-0.54\t\n",
      "[ 0/20][   2400][   14.79s]\td_loss/g_loss:  6.6/-0.65\t\n",
      "[ 0/20][   2500][   15.38s]\td_loss/g_loss:  5.9/-0.32\t\n",
      "[ 0/20][   2600][   15.99s]\td_loss/g_loss:  5.2/-0.67\t\n",
      "[ 0/20][   2700][   16.64s]\td_loss/g_loss:  4.5/-0.86\t\n",
      "[ 0/20][   2800][   17.15s]\td_loss/g_loss:  5.0/-0.88\t\n",
      "[ 0/20][   2900][   17.71s]\td_loss/g_loss:  4.4/-0.52\t\n",
      "[ 0/20][   3000][   18.22s]\td_loss/g_loss:  5.6/-0.73\t\n",
      "[ 0/20][   3100][   18.79s]\td_loss/g_loss:  5.1/-0.67\t\n",
      "[ 0/20][   3200][   19.37s]\td_loss/g_loss:  3.8/-0.6\t\n",
      "[ 0/20][   3300][   19.98s]\td_loss/g_loss:  5.2/-0.82\t\n",
      "[ 0/20][   3400][   20.58s]\td_loss/g_loss:  4.1/-0.83\t\n",
      "[ 0/20][   3500][   21.20s]\td_loss/g_loss:  4.2/-0.56\t\n",
      "[ 0/20][   3600][   21.96s]\td_loss/g_loss:  4.2/-0.49\t\n",
      "[ 0/20][   3700][   22.70s]\td_loss/g_loss:  6.0/-0.63\t\n",
      "[ 0/20][   3800][   23.35s]\td_loss/g_loss:  4.9/-0.56\t\n",
      "[ 0/20][   3900][   24.20s]\td_loss/g_loss:  4.3/-0.43\t\n",
      "[ 0/20][   4000][   24.84s]\td_loss/g_loss:  4.6/-0.38\t\n",
      "[ 0/20][   4100][   25.44s]\td_loss/g_loss:  4.2/-0.6\t\n",
      "[ 0/20][   4200][   26.04s]\td_loss/g_loss:  4.8/-0.66\t\n",
      "[ 0/20][   4300][   26.72s]\td_loss/g_loss:  4.3/-0.63\t\n",
      "[ 0/20][   4400][   27.35s]\td_loss/g_loss:  4.7/-0.67\t\n",
      "[ 0/20][   4500][   27.98s]\td_loss/g_loss:  4.0/-0.75\t\n",
      "[ 0/20][   4600][   28.68s]\td_loss/g_loss:  4.2/-0.66\t\n",
      "[ 0/20][   4700][   29.26s]\td_loss/g_loss:  3.8/-0.61\t\n",
      "[ 0/20][   4800][   29.89s]\td_loss/g_loss:  4.2/-0.66\t\n",
      "[ 0/20][   4900][   30.58s]\td_loss/g_loss:  4.2/-0.47\t\n",
      "[ 0/20][   5000][   31.17s]\td_loss/g_loss:  4.9/-0.83\t\n",
      "[ 0/20][   5100][   31.72s]\td_loss/g_loss:  3.3/-0.49\t\n",
      "[ 0/20][   5200][   32.26s]\td_loss/g_loss:  3.3/-0.78\t\n",
      "[ 0/20][   5300][   32.89s]\td_loss/g_loss:  3.3/-0.83\t\n",
      "[ 0/20][   5400][   33.57s]\td_loss/g_loss:  3.4/-0.61\t\n",
      "[ 0/20][   5500][   34.15s]\td_loss/g_loss:  4.7/-0.71\t\n",
      "[ 0/20][   5600][   34.74s]\td_loss/g_loss:  4.1/-0.47\t\n",
      "[ 0/20][   5700][   35.34s]\td_loss/g_loss:  4.9/-0.44\t\n",
      "[ 0/20][   5800][   35.97s]\td_loss/g_loss:  4.7/-0.34\t\n",
      "[ 0/20][   5900][   36.53s]\td_loss/g_loss:  4.2/-0.61\t\n",
      "[ 0/20][   6000][   37.09s]\td_loss/g_loss:  3.8/-0.44\t\n",
      "[ 0/20][   6100][   37.66s]\td_loss/g_loss:  3.4/-0.42\t\n",
      "[ 0/20][   6200][   38.23s]\td_loss/g_loss:  4.8/-0.48\t\n",
      "[ 0/20][   6300][   38.77s]\td_loss/g_loss:  3.6/-0.57\t\n",
      "[ 0/20][   6400][   39.37s]\td_loss/g_loss:  4.3/-0.72\t\n",
      "[ 0/20][   6500][   40.03s]\td_loss/g_loss:  4.2/-0.31\t\n",
      "[ 0/20][   6600][   40.61s]\td_loss/g_loss:  3.5/-0.78\t\n",
      "[ 0/20][   6700][   41.20s]\td_loss/g_loss:  3.1/-0.57\t\n",
      "[ 0/20][   6800][   41.78s]\td_loss/g_loss:  3.8/-0.36\t\n",
      "[ 0/20][   6900][   42.50s]\td_loss/g_loss:  3.0/-0.51\t\n",
      "[ 0/20][   7000][   43.03s]\td_loss/g_loss:  3.6/-0.56\t\n",
      "[ 0/20][   7100][   43.64s]\td_loss/g_loss:  3.6/-0.49\t\n",
      "[ 0/20][   7200][   44.40s]\td_loss/g_loss:  3.5/-0.63\t\n",
      "[ 0/20][   7300][   44.96s]\td_loss/g_loss:  3.1/-0.61\t\n",
      "[ 0/20][   7400][   45.52s]\td_loss/g_loss:  4.3/-0.7\t\n",
      "[ 0/20][   7500][   45.99s]\td_loss/g_loss:  2.6/-0.63\t\n",
      "[ 0/20][   7600][   46.75s]\td_loss/g_loss:  2.5/-0.65\t\n",
      "[ 0/20][   7700][   47.33s]\td_loss/g_loss:  2.9/-0.72\t\n",
      "[ 0/20][   7800][   48.10s]\td_loss/g_loss:  4.1/-0.86\t\n",
      "[ 0/20][   7900][   48.84s]\td_loss/g_loss:  3.8/-0.41\t\n",
      "[ 0/20][   8000][   49.62s]\td_loss/g_loss:  3.0/-0.57\t\n",
      "[ 0/20][   8100][   50.31s]\td_loss/g_loss:  4.1/-0.68\t\n",
      "[ 0/20][   8200][   50.93s]\td_loss/g_loss:  2.5/-0.67\t\n",
      "[ 0/20][   8300][   51.51s]\td_loss/g_loss:  2.6/-0.74\t\n",
      "[ 0/20][   8400][   52.19s]\td_loss/g_loss:  4.1/-0.42\t\n",
      "[ 0/20][   8500][   52.89s]\td_loss/g_loss:  2.9/-0.38\t\n",
      "[ 0/20][   8600][   53.63s]\td_loss/g_loss:  3.2/-0.72\t\n",
      "[ 0/20][   8700][   54.37s]\td_loss/g_loss:  2.7/-0.7\t\n",
      "[ 0/20][   8800][   55.11s]\td_loss/g_loss:  2.9/-0.75\t\n",
      "[ 0/20][   8900][   55.96s]\td_loss/g_loss:  3.7/-0.4\t\n",
      "[ 0/20][   9000][   56.78s]\td_loss/g_loss:  3.8/-0.77\t\n",
      "[ 0/20][   9100][   57.36s]\td_loss/g_loss:  3.2/-0.7\t\n",
      "[ 0/20][   9200][   57.94s]\td_loss/g_loss:  2.9/-0.42\t\n",
      "[ 0/20][   9300][   58.64s]\td_loss/g_loss:  2.5/-0.7\t\n",
      "[ 0/20][   9400][   59.22s]\td_loss/g_loss:  3.1/-0.6\t\n",
      "[ 0/20][   9500][   59.90s]\td_loss/g_loss:  2.1/-0.39\t\n",
      "[ 0/20][   9600][   60.58s]\td_loss/g_loss:  3.1/-0.74\t\n",
      "[ 0/20][   9700][   61.21s]\td_loss/g_loss:  3.1/-0.56\t\n",
      "[ 0/20][   9800][   61.76s]\td_loss/g_loss:  2.6/-0.68\t\n",
      "[ 0/20][   9900][   62.28s]\td_loss/g_loss:  2.3/-0.47\t\n",
      "[ 0/20][  10000][   62.86s]\td_loss/g_loss:  1.5/-0.58\t\n",
      "[ 0/20][  10100][   63.50s]\td_loss/g_loss:  2.1/-0.35\t\n",
      "[ 0/20][  10200][   64.02s]\td_loss/g_loss:  2.1/-0.64\t\n",
      "[ 0/20][  10300][   64.61s]\td_loss/g_loss:  3.3/-0.44\t\n",
      "[ 0/20][  10400][   65.07s]\td_loss/g_loss:  2.3/-0.34\t\n",
      "[ 0/20][  10500][   65.62s]\td_loss/g_loss:  2.6/-0.38\t\n",
      "[ 0/20][  10600][   66.16s]\td_loss/g_loss:  2.1/-0.46\t\n",
      "[ 0/20][  10700][   66.89s]\td_loss/g_loss:  2.6/-0.4\t\n",
      "[ 0/20][  10800][   67.64s]\td_loss/g_loss:  1.8/-0.44\t\n",
      "[ 0/20][  10900][   68.40s]\td_loss/g_loss:  2.6/-0.78\t\n",
      "[ 0/20][  11000][   69.47s]\td_loss/g_loss:  4.4/-0.67\t\n",
      "[ 0/20][  11100][   70.36s]\td_loss/g_loss:  3.1/-0.54\t\n",
      "[ 0/20][  11200][   71.16s]\td_loss/g_loss:  2.0/-0.5\t\n",
      "[ 0/20][  11300][   72.00s]\td_loss/g_loss:  2.9/-0.43\t\n",
      "[ 0/20][  11400][   72.69s]\td_loss/g_loss:  2.8/-0.56\t\n",
      "[ 0/20][  11500][   73.36s]\td_loss/g_loss:  3.5/-0.7\t\n",
      "[ 0/20][  11600][   74.03s]\td_loss/g_loss:  2.9/-0.42\t\n",
      "[ 0/20][  11700][   74.67s]\td_loss/g_loss:  4.1/-0.69\t\n",
      "[ 0/20][  11800][   75.31s]\td_loss/g_loss:  1.4/-0.62\t\n",
      "[ 0/20][  11900][   75.96s]\td_loss/g_loss:  2.1/-0.48\t\n",
      "[ 0/20][  12000][   76.58s]\td_loss/g_loss:  1.7/-0.67\t\n",
      "[ 0/20][  12100][   77.27s]\td_loss/g_loss:  2.2/-0.75\t\n",
      "[ 0/20][  12200][   78.02s]\td_loss/g_loss:  2.6/-0.71\t\n",
      "[ 0/20][  12300][   78.72s]\td_loss/g_loss:  2.2/-0.76\t\n",
      "[ 0/20][  12400][   79.40s]\td_loss/g_loss:  2.9/-0.69\t\n",
      "[ 0/20][  12500][   80.11s]\td_loss/g_loss:  2.8/-0.43\t\n",
      "[ 0/20][  12600][   80.94s]\td_loss/g_loss:  3.3/-0.69\t\n",
      "[ 0/20][  12700][   81.71s]\td_loss/g_loss:  1.5/-0.43\t\n",
      "[ 0/20][  12800][   82.57s]\td_loss/g_loss:  1.6/-0.46\t\n",
      "[ 0/20][  12900][   83.26s]\td_loss/g_loss:  1.7/-0.69\t\n",
      "[ 0/20][  13000][   83.87s]\td_loss/g_loss:  2.4/-0.75\t\n",
      "[ 0/20][  13100][   84.55s]\td_loss/g_loss:  2.4/-0.58\t\n",
      "[ 0/20][  13200][   85.22s]\td_loss/g_loss:  2.0/-0.41\t\n",
      "[ 0/20][  13300][   85.91s]\td_loss/g_loss:  2.8/-0.62\t\n",
      "[ 0/20][  13400][   86.49s]\td_loss/g_loss:  2.8/-0.72\t\n",
      "[ 0/20][  13500][   87.12s]\td_loss/g_loss:  1.4/-0.51\t\n",
      "[ 0/20][  13600][   87.64s]\td_loss/g_loss:  1.4/-0.55\t\n",
      "[ 0/20][  13700][   88.24s]\td_loss/g_loss:  1.7/-0.68\t\n",
      "[ 0/20][  13800][   88.82s]\td_loss/g_loss:  1.1/-0.66\t\n",
      "[ 0/20][  13900][   89.39s]\td_loss/g_loss:  1.4/-0.48\t\n",
      "[ 0/20][  14000][   89.95s]\td_loss/g_loss:  1.6/-0.7\t\n",
      "[ 0/20][  14100][   90.53s]\td_loss/g_loss:  1.3/-0.54\t\n",
      "[ 0/20][  14200][   91.09s]\td_loss/g_loss:  2.8/-0.72\t\n",
      "[ 0/20][  14300][   91.73s]\td_loss/g_loss:  1.4/-0.44\t\n",
      "[ 0/20][  14400][   92.42s]\td_loss/g_loss:  2.1/-0.71\t\n",
      "[ 0/20][  14500][   93.06s]\td_loss/g_loss:  1.7/-0.54\t\n",
      "[ 0/20][  14600][   93.68s]\td_loss/g_loss:  1.5/-0.66\t\n",
      "[ 0/20][  14700][   94.29s]\td_loss/g_loss:  2.4/-0.61\t\n",
      "[ 0/20][  14800][   94.86s]\td_loss/g_loss:  1.5/-0.63\t\n",
      "[ 0/20][  14900][   95.47s]\td_loss/g_loss:  2.4/-0.47\t\n",
      "[ 0/20][  15000][   96.05s]\td_loss/g_loss:  1.6/-0.64\t\n",
      "[ 0/20][  15100][   96.71s]\td_loss/g_loss:  2.7/-0.67\t\n",
      "[ 0/20][  15200][   97.38s]\td_loss/g_loss:  1.9/-0.63\t\n",
      "[ 0/20][  15300][   98.03s]\td_loss/g_loss:  2.3/-0.63\t\n",
      "[ 0/20][  15400][   98.66s]\td_loss/g_loss:  1.9/-0.53\t\n",
      "[ 0/20][  15500][   99.20s]\td_loss/g_loss:  1.1/-0.75\t\n",
      "[ 0/20][  15600][   99.83s]\td_loss/g_loss:  1.7/-0.53\t\n",
      "[ 0/20][  15700][  100.45s]\td_loss/g_loss:  1.7/-0.56\t\n",
      "[ 0/20][  15800][  101.02s]\td_loss/g_loss:  1.4/-0.51\t\n",
      "[ 0/20][  15900][  101.60s]\td_loss/g_loss:  1.9/-0.43\t\n",
      "[ 0/20][  16000][  102.19s]\td_loss/g_loss: 0.99/-0.47\t\n",
      "[ 0/20][  16100][  102.82s]\td_loss/g_loss:  1.8/-0.47\t\n",
      "[ 0/20][  16200][  103.47s]\td_loss/g_loss:  1.5/-0.48\t\n",
      "[ 0/20][  16300][  104.11s]\td_loss/g_loss:  2.8/-0.64\t\n",
      "[ 0/20][  16400][  104.77s]\td_loss/g_loss:  1.2/-0.7\t\n",
      "[ 0/20][  16500][  105.36s]\td_loss/g_loss:  2.2/-0.61\t\n",
      "[ 0/20][  16600][  106.01s]\td_loss/g_loss:  1.1/-0.75\t\n",
      "[ 0/20][  16700][  106.53s]\td_loss/g_loss:  1.8/-0.63\t\n",
      "[ 0/20][  16800][  107.01s]\td_loss/g_loss:  3.0/-0.75\t\n",
      "[ 0/20][  16900][  107.55s]\td_loss/g_loss:  1.1/-0.57\t\n",
      "[ 0/20][  17000][  108.19s]\td_loss/g_loss:  1.4/-0.41\t\n",
      "[ 0/20][  17100][  108.77s]\td_loss/g_loss:  1.2/-0.67\t\n",
      "[ 0/20][  17200][  109.35s]\td_loss/g_loss:  1.5/-0.55\t\n",
      "[ 0/20][  17300][  109.92s]\td_loss/g_loss: 0.98/-0.68\t\n",
      "[ 0/20][  17400][  110.67s]\td_loss/g_loss:  1.0/-0.48\t\n",
      "[ 0/20][  17500][  111.42s]\td_loss/g_loss:  2.3/-0.64\t\n",
      "[ 0/20][  17600][  112.10s]\td_loss/g_loss:  1.1/-0.43\t\n",
      "[ 0/20][  17700][  112.68s]\td_loss/g_loss:  2.6/-0.58\t\n",
      "[ 0/20][  17800][  113.29s]\td_loss/g_loss:  1.4/-0.7\t\n",
      "[ 0/20][  17900][  113.87s]\td_loss/g_loss:  1.3/-0.55\t\n",
      "[ 0/20][  18000][  114.49s]\td_loss/g_loss:  1.2/-0.72\t\n",
      "[ 0/20][  18100][  115.13s]\td_loss/g_loss:  1.3/-0.51\t\n",
      "[ 0/20][  18200][  115.69s]\td_loss/g_loss:  1.3/-0.45\t\n",
      "[ 0/20][  18300][  116.27s]\td_loss/g_loss: 0.63/-0.62\t\n",
      "[ 0/20][  18400][  116.85s]\td_loss/g_loss:  2.2/-0.47\t\n",
      "[ 0/20][  18500][  117.32s]\td_loss/g_loss:  1.9/-0.72\t\n",
      "[ 0/20][  18600][  118.09s]\td_loss/g_loss:  1.4/-0.47\t\n",
      "[ 0/20][  18700][  118.64s]\td_loss/g_loss:  2.3/-0.71\t\n",
      "[ 0/20][  18800][  119.41s]\td_loss/g_loss:  0.5/-0.57\t\n",
      "[ 0/20][  18900][  120.05s]\td_loss/g_loss:  1.3/-0.64\t\n",
      "[ 0/20][  19000][  120.60s]\td_loss/g_loss: 0.82/-0.56\t\n",
      "[ 0/20][  19100][  121.22s]\td_loss/g_loss:  1.2/-0.74\t\n",
      "[ 0/20][  19200][  121.93s]\td_loss/g_loss:  1.9/-0.65\t\n",
      "[ 0/20][  19300][  122.72s]\td_loss/g_loss: 0.83/-0.64\t\n",
      "[ 0/20][  19400][  123.65s]\td_loss/g_loss: 0.95/-0.57\t\n",
      "[ 0/20][  19500][  124.55s]\td_loss/g_loss:  1.0/-0.69\t\n",
      "[ 0/20][  19600][  125.59s]\td_loss/g_loss:  2.0/-0.77\t\n",
      "[ 0/20][  19700][  126.53s]\td_loss/g_loss:  2.3/-0.7\t\n",
      "[ 0/20][  19800][  127.54s]\td_loss/g_loss: 0.83/-0.69\t\n",
      "[ 0/20][  19900][  128.53s]\td_loss/g_loss:  1.9/-0.77\t\n",
      "[ 0/20][  20000][  129.21s]\td_loss/g_loss: 0.82/-0.78\t\n",
      "[ 0/20][  20100][  129.85s]\td_loss/g_loss:  2.7/-0.77\t\n",
      "[ 0/20][  20200][  130.84s]\td_loss/g_loss:  1.5/-0.74\t\n",
      "[ 0/20][  20300][  131.66s]\td_loss/g_loss: 0.87/-0.75\t\n",
      "[ 0/20][  20400][  132.37s]\td_loss/g_loss:  3.0/-0.78\t\n",
      "[ 0/20][  20500][  133.42s]\td_loss/g_loss:  1.7/-0.66\t\n",
      "[ 0/20][  20600][  134.32s]\td_loss/g_loss:  2.2/-0.6\t\n",
      "[ 0/20][  20700][  135.33s]\td_loss/g_loss:  1.7/-0.59\t\n",
      "[ 0/20][  20800][  136.28s]\td_loss/g_loss:  1.8/-0.62\t\n",
      "[ 0/20][  20900][  137.04s]\td_loss/g_loss:  2.2/-0.68\t\n",
      "[ 0/20][  21000][  137.67s]\td_loss/g_loss:  2.5/-0.63\t\n",
      "[ 0/20][  21100][  138.26s]\td_loss/g_loss: 0.95/-0.63\t\n",
      "[ 0/20][  21200][  138.99s]\td_loss/g_loss:  5.2/-0.71\t\n",
      "[ 0/20][  21300][  139.65s]\td_loss/g_loss: 0.83/-0.5\t\n",
      "[ 0/20][  21400][  140.28s]\td_loss/g_loss:  1.3/-0.61\t\n",
      "[ 0/20][  21500][  140.93s]\td_loss/g_loss:  1.0/-0.68\t\n",
      "[ 0/20][  21600][  141.59s]\td_loss/g_loss:  1.7/-0.5\t\n",
      "[ 0/20][  21700][  142.23s]\td_loss/g_loss: 0.83/-0.66\t\n",
      "[ 0/20][  21800][  142.83s]\td_loss/g_loss: 0.87/-0.59\t\n",
      "[ 0/20][  21900][  143.45s]\td_loss/g_loss:  0.7/-0.61\t\n",
      "[ 0/20][  22000][  144.08s]\td_loss/g_loss:  1.2/-0.51\t\n",
      "[ 0/20][  22100][  144.73s]\td_loss/g_loss: 0.89/-0.62\t\n",
      "[ 0/20][  22200][  145.42s]\td_loss/g_loss:  2.0/-0.62\t\n",
      "[ 0/20][  22300][  146.04s]\td_loss/g_loss:  2.7/-0.65\t\n",
      "[ 0/20][  22400][  146.67s]\td_loss/g_loss:  1.5/-0.47\t\n",
      "[ 0/20][  22500][  147.19s]\td_loss/g_loss:  2.1/-0.63\t\n",
      "[ 0/20][  22600][  147.77s]\td_loss/g_loss:  2.4/-0.62\t\n",
      "[ 0/20][  22700][  148.36s]\td_loss/g_loss:  2.6/-0.66\t\n",
      "[ 0/20][  22800][  149.03s]\td_loss/g_loss:  2.2/-0.6\t\n",
      "[ 0/20][  22900][  149.69s]\td_loss/g_loss: 0.76/-0.65\t\n",
      "[ 0/20][  23000][  150.37s]\td_loss/g_loss:  1.2/-0.52\t\n",
      "[ 0/20][  23100][  151.05s]\td_loss/g_loss:  2.0/-0.58\t\n",
      "[ 0/20][  23200][  151.72s]\td_loss/g_loss:  2.6/-0.6\t\n",
      "[ 0/20][  23300][  152.29s]\td_loss/g_loss: 0.88/-0.52\t\n",
      "[ 0/20][  23400][  152.96s]\td_loss/g_loss: 0.63/-0.63\t\n",
      "[ 0/20][  23500][  153.57s]\td_loss/g_loss:  2.2/-0.62\t\n",
      "[ 0/20][  23600][  154.21s]\td_loss/g_loss: 0.84/-0.69\t\n",
      "[ 0/20][  23700][  154.86s]\td_loss/g_loss:  2.5/-0.55\t\n",
      "[ 0/20][  23800][  155.53s]\td_loss/g_loss:  2.5/-0.6\t\n",
      "[ 0/20][  23900][  156.19s]\td_loss/g_loss: 0.56/-0.65\t\n",
      "[ 0/20][  24000][  156.89s]\td_loss/g_loss:  0.6/-0.53\t\n",
      "[ 0/20][  24100][  157.57s]\td_loss/g_loss: 0.85/-0.68\t\n",
      "[ 0/20][  24200][  158.26s]\td_loss/g_loss: 0.79/-0.67\t\n",
      "[ 0/20][  24300][  158.94s]\td_loss/g_loss:  2.9/-0.63\t\n",
      "[ 0/20][  24400][  159.62s]\td_loss/g_loss: 0.77/-0.64\t\n",
      "[ 0/20][  24500][  160.24s]\td_loss/g_loss:  1.1/-0.58\t\n",
      "[ 0/20][  24600][  160.87s]\td_loss/g_loss:  1.0/-0.63\t\n",
      "[ 0/20][  24700][  161.53s]\td_loss/g_loss:  2.7/-0.63\t\n",
      "[ 0/20][  24800][  162.20s]\td_loss/g_loss:  1.8/-0.63\t\n",
      "[ 0/20][  24900][  162.85s]\td_loss/g_loss:  1.0/-0.59\t\n",
      "[ 0/20][  25000][  163.49s]\td_loss/g_loss: 0.83/-0.62\t\n",
      "[ 0/20][  25100][  164.15s]\td_loss/g_loss: 0.78/-0.59\t\n",
      "[ 0/20][  25200][  164.76s]\td_loss/g_loss:  1.4/-0.44\t\n",
      "[ 0/20][  25300][  165.39s]\td_loss/g_loss: 0.87/-0.6\t\n",
      "[ 0/20][  25400][  166.07s]\td_loss/g_loss: 0.66/-0.64\t\n",
      "[ 0/20][  25500][  166.69s]\td_loss/g_loss: 0.78/-0.58\t\n",
      "[ 0/20][  25600][  167.33s]\td_loss/g_loss: 0.85/-0.53\t\n",
      "[ 0/20][  25700][  167.97s]\td_loss/g_loss: 0.73/-0.53\t\n",
      "[ 0/20][  25800][  168.69s]\td_loss/g_loss: 0.95/-0.49\t\n",
      "[ 0/20][  25900][  169.32s]\td_loss/g_loss: 0.83/-0.6\t\n",
      "[ 0/20][  26000][  169.92s]\td_loss/g_loss: 0.76/-0.62\t\n",
      "[ 0/20][  26100][  170.52s]\td_loss/g_loss:  1.5/-0.53\t\n",
      "[ 0/20][  26200][  171.27s]\td_loss/g_loss: 0.58/-0.56\t\n",
      "[ 0/20][  26300][  171.96s]\td_loss/g_loss: 0.81/-0.63\t\n",
      "[ 0/20][  26400][  172.66s]\td_loss/g_loss: 0.69/-0.6\t\n",
      "[ 0/20][  26500][  173.29s]\td_loss/g_loss:  1.3/-0.62\t\n",
      "[ 0/20][  26600][  173.89s]\td_loss/g_loss: 0.69/-0.56\t\n",
      "[ 0/20][  26700][  174.52s]\td_loss/g_loss: 0.98/-0.52\t\n",
      "[ 0/20][  26800][  175.02s]\td_loss/g_loss: 0.88/-0.56\t\n",
      "[ 0/20][  26900][  175.63s]\td_loss/g_loss: 0.61/-0.54\t\n",
      "[ 0/20][  27000][  176.22s]\td_loss/g_loss:  2.1/-0.66\t\n",
      "[ 0/20][  27100][  176.73s]\td_loss/g_loss:  1.5/-0.54\t\n",
      "[ 0/20][  27200][  177.29s]\td_loss/g_loss:  2.5/-0.6\t\n",
      "[ 0/20][  27300][  177.86s]\td_loss/g_loss:  2.1/-0.59\t\n",
      "[ 0/20][  27400][  178.35s]\td_loss/g_loss:  2.3/-0.56\t\n",
      "[ 0/20][  27500][  178.90s]\td_loss/g_loss: 0.96/-0.63\t\n",
      "[ 0/20][  27600][  179.46s]\td_loss/g_loss: 0.94/-0.52\t\n",
      "[ 0/20][  27700][  180.08s]\td_loss/g_loss: 0.82/-0.6\t\n",
      "[ 0/20][  27800][  180.69s]\td_loss/g_loss:  1.0/-0.54\t\n",
      "[ 0/20][  27900][  181.27s]\td_loss/g_loss:  2.9/-0.55\t\n",
      "[ 0/20][  28000][  181.74s]\td_loss/g_loss:  1.4/-0.6\t\n",
      "[ 0/20][  28100][  182.24s]\td_loss/g_loss: 0.82/-0.54\t\n",
      "[ 0/20][  28200][  182.78s]\td_loss/g_loss:  0.6/-0.55\t\n",
      "[ 0/20][  28300][  183.35s]\td_loss/g_loss: 0.92/-0.55\t\n",
      "[ 0/20][  28400][  183.87s]\td_loss/g_loss:  3.0/-0.56\t\n",
      "[ 0/20][  28500][  184.37s]\td_loss/g_loss:  1.3/-0.6\t\n",
      "[ 0/20][  28600][  184.97s]\td_loss/g_loss: 0.88/-0.57\t\n",
      "[ 0/20][  28700][  185.59s]\td_loss/g_loss:  2.6/-0.61\t\n",
      "[ 0/20][  28800][  186.23s]\td_loss/g_loss:  0.6/-0.63\t\n",
      "[ 0/20][  28900][  186.83s]\td_loss/g_loss: 0.87/-0.53\t\n",
      "[ 0/20][  29000][  187.51s]\td_loss/g_loss:  0.7/-0.58\t\n",
      "[ 0/20][  29100][  188.15s]\td_loss/g_loss:  2.1/-0.6\t\n",
      "[ 0/20][  29200][  188.76s]\td_loss/g_loss: 0.85/-0.53\t\n",
      "[ 0/20][  29300][  189.34s]\td_loss/g_loss:  2.5/-0.58\t\n",
      "[ 0/20][  29400][  189.88s]\td_loss/g_loss: 0.53/-0.58\t\n",
      "[ 0/20][  29500][  190.48s]\td_loss/g_loss: 0.36/-0.6\t\n",
      "[ 0/20][  29600][  191.07s]\td_loss/g_loss:  1.3/-0.44\t\n",
      "[ 0/20][  29700][  191.65s]\td_loss/g_loss: 0.52/-0.6\t\n",
      "[ 0/20][  29800][  192.08s]\td_loss/g_loss: 0.76/-0.54\t\n",
      "[ 0/20][  29900][  192.53s]\td_loss/g_loss:  2.2/-0.54\t\n",
      "[ 0/20][  30000][  193.01s]\td_loss/g_loss: 0.52/-0.63\t\n",
      "[ 0/20][  30100][  193.48s]\td_loss/g_loss:  0.8/-0.58\t\n",
      "[ 0/20][  30200][  194.00s]\td_loss/g_loss: 0.68/-0.6\t\n",
      "[ 0/20][  30300][  194.46s]\td_loss/g_loss:  1.1/-0.54\t\n",
      "[ 0/20][  30400][  195.03s]\td_loss/g_loss:  1.6/-0.58\t\n",
      "[ 0/20][  30500][  195.52s]\td_loss/g_loss: 0.47/-0.6\t\n",
      "[ 0/20][  30600][  196.08s]\td_loss/g_loss: 0.71/-0.62\t\n",
      "[ 0/20][  30700][  196.76s]\td_loss/g_loss:  1.6/-0.49\t\n",
      "[ 0/20][  30800][  197.35s]\td_loss/g_loss: 0.72/-0.59\t\n",
      "[ 0/20][  30900][  197.94s]\td_loss/g_loss: 0.98/-0.55\t\n",
      "[ 0/20][  31000][  198.61s]\td_loss/g_loss:  1.6/-0.57\t\n",
      "[ 0/20][  31100][  199.32s]\td_loss/g_loss: 0.49/-0.59\t\n",
      "[ 0/20][  31200][  199.85s]\td_loss/g_loss:  1.0/-0.58\t\n",
      "[ 0/20][  31300][  200.47s]\td_loss/g_loss:  1.2/-0.51\t\n",
      "[ 0/20][  31400][  201.09s]\td_loss/g_loss:  0.7/-0.63\t\n",
      "[ 0/20][  31500][  201.71s]\td_loss/g_loss:  3.2/-0.41\t\n",
      "[ 0/20][  31600][  202.38s]\td_loss/g_loss: 0.55/-0.61\t\n",
      "[ 0/20][  31700][  203.01s]\td_loss/g_loss:  2.4/-0.47\t\n",
      "[ 0/20][  31800][  203.67s]\td_loss/g_loss: 0.95/-0.54\t\n",
      "[ 0/20][  31900][  204.31s]\td_loss/g_loss:  0.5/-0.55\t\n",
      "[ 1/20][  31987][  205.20s]\td_loss/g_loss: 0.43/-0.55\t\n",
      "[ 1/20][  32087][  205.87s]\td_loss/g_loss: 0.54/-0.54\t\n",
      "[ 1/20][  32187][  206.53s]\td_loss/g_loss: 0.92/-0.62\t\n",
      "[ 1/20][  32287][  207.32s]\td_loss/g_loss:  1.9/-0.59\t\n",
      "[ 1/20][  32387][  207.94s]\td_loss/g_loss:  1.2/-0.52\t\n",
      "[ 1/20][  32487][  208.61s]\td_loss/g_loss:  0.6/-0.58\t\n",
      "[ 1/20][  32587][  209.21s]\td_loss/g_loss:  2.7/-0.53\t\n",
      "[ 1/20][  32687][  209.75s]\td_loss/g_loss: 0.45/-0.57\t\n",
      "[ 1/20][  32787][  210.26s]\td_loss/g_loss: 0.46/-0.55\t\n",
      "[ 1/20][  32887][  210.77s]\td_loss/g_loss: 0.57/-0.57\t\n",
      "[ 1/20][  32987][  211.31s]\td_loss/g_loss:  1.3/-0.56\t\n",
      "[ 1/20][  33087][  211.94s]\td_loss/g_loss:  2.0/-0.52\t\n",
      "[ 1/20][  33187][  212.52s]\td_loss/g_loss: 0.53/-0.59\t\n",
      "[ 1/20][  33287][  213.05s]\td_loss/g_loss:  1.9/-0.52\t\n",
      "[ 1/20][  33387][  213.51s]\td_loss/g_loss:  1.1/-0.62\t\n",
      "[ 1/20][  33487][  214.02s]\td_loss/g_loss:  2.4/-0.51\t\n",
      "[ 1/20][  33587][  214.55s]\td_loss/g_loss:  3.0/-0.42\t\n",
      "[ 1/20][  33687][  215.10s]\td_loss/g_loss:  1.4/-0.53\t\n",
      "[ 1/20][  33787][  215.73s]\td_loss/g_loss: 0.53/-0.58\t\n",
      "[ 1/20][  33887][  216.37s]\td_loss/g_loss: 0.44/-0.49\t\n",
      "[ 1/20][  33987][  217.00s]\td_loss/g_loss:  2.2/-0.55\t\n",
      "[ 1/20][  34087][  217.64s]\td_loss/g_loss: 0.52/-0.56\t\n",
      "[ 1/20][  34187][  218.26s]\td_loss/g_loss:  1.0/-0.46\t\n",
      "[ 1/20][  34287][  218.82s]\td_loss/g_loss:  0.7/-0.56\t\n",
      "[ 1/20][  34387][  219.42s]\td_loss/g_loss: 0.98/-0.56\t\n",
      "[ 1/20][  34487][  220.06s]\td_loss/g_loss: 0.43/-0.59\t\n",
      "[ 1/20][  34587][  220.78s]\td_loss/g_loss: 0.71/-0.54\t\n",
      "[ 1/20][  34687][  221.44s]\td_loss/g_loss:  2.3/-0.45\t\n",
      "[ 1/20][  34787][  222.02s]\td_loss/g_loss: 0.51/-0.56\t\n",
      "[ 1/20][  34887][  222.60s]\td_loss/g_loss:  1.8/-0.39\t\n",
      "[ 1/20][  34987][  223.12s]\td_loss/g_loss:  0.5/-0.61\t\n",
      "[ 1/20][  35087][  223.67s]\td_loss/g_loss: 0.81/-0.54\t\n",
      "[ 1/20][  35187][  224.21s]\td_loss/g_loss: 0.42/-0.53\t\n",
      "[ 1/20][  35287][  224.76s]\td_loss/g_loss: 0.44/-0.58\t\n",
      "[ 1/20][  35387][  225.25s]\td_loss/g_loss:  1.4/-0.46\t\n",
      "[ 1/20][  35487][  225.72s]\td_loss/g_loss: 0.95/-0.56\t\n",
      "[ 1/20][  35587][  226.34s]\td_loss/g_loss:  1.7/-0.45\t\n",
      "[ 1/20][  35687][  227.03s]\td_loss/g_loss: 0.85/-0.51\t\n",
      "[ 1/20][  35787][  227.72s]\td_loss/g_loss: 0.53/-0.52\t\n",
      "[ 1/20][  35887][  228.57s]\td_loss/g_loss: 0.67/-0.47\t\n",
      "[ 1/20][  35987][  229.23s]\td_loss/g_loss:  0.8/-0.51\t\n",
      "[ 1/20][  36087][  229.87s]\td_loss/g_loss: 0.33/-0.55\t\n",
      "[ 1/20][  36187][  230.45s]\td_loss/g_loss: 0.48/-0.52\t\n",
      "[ 1/20][  36287][  231.05s]\td_loss/g_loss:  2.0/-0.43\t\n",
      "[ 1/20][  36387][  231.70s]\td_loss/g_loss: 0.62/-0.5\t\n",
      "[ 1/20][  36487][  232.24s]\td_loss/g_loss: 0.88/-0.5\t\n",
      "[ 1/20][  36587][  232.77s]\td_loss/g_loss: 0.48/-0.54\t\n",
      "[ 1/20][  36687][  233.30s]\td_loss/g_loss: 0.83/-0.46\t\n",
      "[ 1/20][  36787][  233.98s]\td_loss/g_loss: 0.64/-0.42\t\n",
      "[ 1/20][  36887][  234.58s]\td_loss/g_loss:  0.6/-0.57\t\n",
      "[ 1/20][  36987][  235.13s]\td_loss/g_loss: 0.62/-0.57\t\n",
      "[ 1/20][  37087][  235.69s]\td_loss/g_loss:  4.6/-0.44\t\n",
      "[ 1/20][  37187][  236.20s]\td_loss/g_loss: 0.42/-0.59\t\n",
      "[ 1/20][  37287][  236.71s]\td_loss/g_loss: 0.41/-0.57\t\n",
      "[ 1/20][  37387][  237.34s]\td_loss/g_loss: 0.41/-0.54\t\n",
      "[ 1/20][  37487][  237.93s]\td_loss/g_loss: 0.47/-0.56\t\n",
      "[ 1/20][  37587][  238.53s]\td_loss/g_loss: 0.65/-0.5\t\n",
      "[ 1/20][  37687][  239.10s]\td_loss/g_loss:  1.1/-0.43\t\n",
      "[ 1/20][  37787][  239.62s]\td_loss/g_loss: 0.74/-0.54\t\n",
      "[ 1/20][  37887][  240.28s]\td_loss/g_loss:  2.4/-0.46\t\n",
      "[ 1/20][  37987][  240.88s]\td_loss/g_loss: 0.99/-0.46\t\n",
      "[ 1/20][  38087][  241.58s]\td_loss/g_loss:  1.1/-0.54\t\n",
      "[ 1/20][  38187][  242.15s]\td_loss/g_loss: 0.49/-0.58\t\n",
      "[ 1/20][  38287][  242.69s]\td_loss/g_loss: 0.39/-0.52\t\n",
      "[ 1/20][  38387][  243.25s]\td_loss/g_loss:  2.0/-0.48\t\n",
      "[ 1/20][  38487][  243.84s]\td_loss/g_loss:  1.0/-0.56\t\n",
      "[ 1/20][  38587][  244.42s]\td_loss/g_loss: 0.33/-0.58\t\n",
      "[ 1/20][  38687][  245.05s]\td_loss/g_loss:  0.4/-0.6\t\n",
      "[ 1/20][  38787][  245.70s]\td_loss/g_loss: 0.61/-0.55\t\n",
      "[ 1/20][  38887][  246.36s]\td_loss/g_loss:  1.2/-0.48\t\n",
      "[ 1/20][  38987][  247.07s]\td_loss/g_loss: 0.68/-0.54\t\n",
      "[ 1/20][  39087][  247.78s]\td_loss/g_loss: 0.45/-0.49\t\n",
      "[ 1/20][  39187][  248.40s]\td_loss/g_loss: 0.34/-0.58\t\n",
      "[ 1/20][  39287][  248.97s]\td_loss/g_loss: 0.77/-0.48\t\n",
      "[ 1/20][  39387][  249.52s]\td_loss/g_loss: 0.45/-0.55\t\n",
      "[ 1/20][  39487][  250.05s]\td_loss/g_loss: 0.71/-0.46\t\n",
      "[ 1/20][  39587][  250.70s]\td_loss/g_loss:  0.8/-0.52\t\n",
      "[ 1/20][  39687][  251.36s]\td_loss/g_loss: 0.29/-0.56\t\n",
      "[ 1/20][  39787][  251.97s]\td_loss/g_loss: 0.46/-0.51\t\n",
      "[ 1/20][  39887][  252.54s]\td_loss/g_loss:  1.4/-0.5\t\n",
      "[ 1/20][  39987][  253.10s]\td_loss/g_loss: 0.66/-0.54\t\n",
      "[ 1/20][  40087][  253.75s]\td_loss/g_loss: 0.41/-0.45\t\n",
      "[ 1/20][  40187][  254.33s]\td_loss/g_loss: 0.66/-0.55\t\n",
      "[ 1/20][  40287][  254.97s]\td_loss/g_loss: 0.42/-0.52\t\n",
      "[ 1/20][  40387][  255.64s]\td_loss/g_loss: 0.52/-0.52\t\n",
      "[ 1/20][  40487][  256.23s]\td_loss/g_loss: 0.48/-0.54\t\n",
      "[ 1/20][  40587][  256.81s]\td_loss/g_loss: 0.53/-0.61\t\n",
      "[ 1/20][  40687][  257.39s]\td_loss/g_loss: 0.42/-0.5\t\n",
      "[ 1/20][  40787][  258.02s]\td_loss/g_loss:  1.3/-0.43\t\n",
      "[ 1/20][  40887][  258.69s]\td_loss/g_loss:  0.4/-0.54\t\n",
      "[ 1/20][  40987][  259.28s]\td_loss/g_loss: 0.28/-0.5\t\n",
      "[ 1/20][  41087][  259.93s]\td_loss/g_loss: 0.38/-0.61\t\n",
      "[ 1/20][  41187][  260.55s]\td_loss/g_loss: 0.35/-0.55\t\n",
      "[ 1/20][  41287][  261.08s]\td_loss/g_loss: 0.55/-0.52\t\n",
      "[ 1/20][  41387][  261.62s]\td_loss/g_loss: 0.57/-0.49\t\n",
      "[ 1/20][  41487][  262.18s]\td_loss/g_loss: 0.49/-0.54\t\n",
      "[ 1/20][  41587][  262.77s]\td_loss/g_loss: 0.43/-0.54\t\n",
      "[ 1/20][  41687][  263.28s]\td_loss/g_loss: 0.33/-0.52\t\n",
      "[ 1/20][  41787][  263.77s]\td_loss/g_loss:  0.3/-0.52\t\n",
      "[ 1/20][  41887][  264.26s]\td_loss/g_loss: 0.33/-0.51\t\n",
      "[ 1/20][  41987][  264.73s]\td_loss/g_loss: 0.83/-0.56\t\n",
      "[ 1/20][  42087][  265.22s]\td_loss/g_loss:  1.4/-0.59\t\n",
      "[ 1/20][  42187][  265.81s]\td_loss/g_loss: 0.95/-0.59\t\n",
      "[ 1/20][  42287][  266.31s]\td_loss/g_loss:  1.2/-0.58\t\n",
      "[ 1/20][  42387][  266.81s]\td_loss/g_loss: 0.78/-0.51\t\n",
      "[ 1/20][  42487][  267.38s]\td_loss/g_loss:  0.8/-0.46\t\n",
      "[ 1/20][  42587][  267.93s]\td_loss/g_loss: 0.31/-0.52\t\n",
      "[ 1/20][  42687][  268.50s]\td_loss/g_loss: 0.42/-0.55\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 30\u001B[0m\n\u001B[1;32m     27\u001B[0m critic_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mcritic_loss_real \u001B[38;5;241m+\u001B[39m critic_loss_fake  \u001B[38;5;241m+\u001B[39m gradient_penalty\n\u001B[1;32m     29\u001B[0m critic_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 30\u001B[0m \u001B[43mcritic_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m hp\u001B[38;5;241m.\u001B[39mn_critic \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Update Generator\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     generator_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:162\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    158\u001B[0m             max_exp_avg_sqs\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    160\u001B[0m         state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 162\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m          \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m          \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m          \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m          \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m          \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m          \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m          \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:219\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 219\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:273\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001B[0m\n\u001B[1;32m    270\u001B[0m param\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m lr \u001B[38;5;241m*\u001B[39m weight_decay)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m    274\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "img_list, generator_losses, critic_losses = [], [], []\n",
    "iters = 0\n",
    "grad_tensor = torch.ones((hp.batchsize, 1), device=\"cuda\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(hp.num_epochs):\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        real_images, real_class_labels = data[0].to(\"cuda\"), data[1].to(\"cuda\")\n",
    "        \n",
    "        # Update critic\n",
    "        critic_optimizer.zero_grad()\n",
    "        \n",
    "        critic_output_real = critic(real_images, real_class_labels)\n",
    "        critic_loss_real = critic_output_real.mean()\n",
    "\n",
    "        noise = torch.randn((hp.batchsize, hp.noise_size), device=\"cuda\")\n",
    "        with torch.no_grad(): fake_image = generator(noise, real_class_labels)\n",
    "        critic_output_fake = critic(fake_image, real_class_labels)\n",
    "        critic_loss_fake = critic_output_fake.mean()\n",
    "\n",
    "        alpha = torch.rand(1, device=\"cuda\")\n",
    "        interpolates = (alpha * real_images + (1. - alpha) * fake_image).requires_grad_(True)\n",
    "        d_interpolates = critic(interpolates, real_class_labels).reshape(-1, 1)\n",
    "        gradients = autograd.grad(d_interpolates, interpolates, grad_tensor, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_penalty = hp.gp_lambda * ((gradients.view(hp.batchsize, -1).norm(dim=1) - 1.) ** 2).mean()\n",
    "\n",
    "        critic_loss = -critic_loss_real + critic_loss_fake  + gradient_penalty\n",
    "        \n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        if batch_idx % hp.n_critic == 0:\n",
    "            # Update Generator\n",
    "            generator_optimizer.zero_grad()\n",
    "            \n",
    "            fake_class_labels = dataset.y_train[torch.randint(high=len(dataset), size=[hp.batchsize])].to(device=\"cuda\")\n",
    "            noise = torch.randn((hp.batchsize, hp.noise_size), device=\"cuda\")\n",
    "            fake_image = generator(noise, fake_class_labels)\n",
    "            critic_output_fake = critic(fake_image, fake_class_labels)\n",
    "            generator_loss = -critic_output_fake.mean()\n",
    "            \n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_idx % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"[{epoch:>2}/{hp.num_epochs}][{iters:>7}][{elapsed_time:8.2f}s]\\t\"\n",
    "                  f\"d_loss/g_loss: {critic_loss.item():4.2}/{generator_loss.item():4.2}\\t\")\n",
    "       \n",
    "        # Save Losses for plotting later\n",
    "        generator_losses.append(generator_loss.item())\n",
    "        critic_losses.append(critic_loss.item())\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JhDPdes7QYyf"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Generator and critic losses during training\")\n",
    "plt.plot(generator_losses,label=\"G\")\n",
    "plt.plot(critic_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
